\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{probRobs}
\citation{ekf}
\citation{SLAM}
\citation{liflonglearning}
\citation{lifelongmaps}
\citation{aishalong}
\citation{nonParam}
\citation{LDA}
\citation{speakerDiar}
\citation{kinect}
\citation{pcl}
\citation{rtabmap}
\citation{rgbdmapping}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}}
\newlabel{sec:Intro}{{1}{5}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{5}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Tools and methods}{5}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Research questions}{6}{subsection.1.3}}
\citation{SLAM++}
\citation{objSLAM}
\citation{objectpointSLAM}
\citation{omnimaper}
\citation{objSLAM}
\citation{monoSLAM}
\citation{objectDisc}
\citation{objectDisc}
\citation{pointSeg}
\citation{pcl}
\citation{planarSeg}
\citation{planarSeg2}
\citation{segOverview}
\citation{smartSeg}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature review}{7}{section.2}}
\newlabel{sec:literature}{{2}{7}{Literature review}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Object based SLAM}{7}{subsection.2.1}}
\citation{bayes:neal}
\citation{bayes:jordan}
\citation{LDA}
\citation{bayes:hier}
\citation{bayes:smc}
\citation{corresp:first}
\citation{corres:sec}
\citation{corres:three}
\citation{corres:four}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Point Cloud Object clustering}{8}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Non Parametric Bayesian methods}{8}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Correspondence}{8}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory background}{10}{section.3}}
\newlabel{sec:theory}{{3}{10}{Theory background}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dirichlet Distribution}{10}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Different behaviour the distribution for different initial parameters of the $\alpha $ vector}}{11}{figure.1}}
\newlabel{firstplot}{{1}{11}{Different behaviour the distribution for different initial parameters of the $\alpha $ vector}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sampling methods}{11}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Stick breaking process}{11}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Realizations from a Dirichlet distribution using the stick breaking construction in $R^3$. Each color represents the weight of the respective component. Weights sum up to 1 making every realization a probability mass function. A single line can be mapped to a single dot in Fig. \ref  {firstplot} }}{12}{figure.2}}
\newlabel{stickR3}{{2}{12}{Realizations from a Dirichlet distribution using the stick breaking construction in $R^3$. Each color represents the weight of the respective component. Weights sum up to 1 making every realization a probability mass function. A single line can be mapped to a single dot in Fig. \ref {firstplot}}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Polya's Urn}{12}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Polya's urn for $\alpha $=[2 1 1] }}{13}{figure.3}}
\newlabel{polysurn}{{3}{13}{Polya's urn for $\alpha $=[2 1 1]}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Dirichlet Process}{13}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Sampling methods}{14}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Chinese restaurant process}{14}{subsubsection.3.4.1}}
\newlabel{sec:crp}{{3.4.1}{14}{Chinese restaurant process}{subsubsection.3.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The CRP process}}{14}{figure.4}}
\newlabel{CRP}{{4}{14}{The CRP process}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Stick breaking}{15}{subsubsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Dirichlet process mixture models}{15}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The stick breaking process. Every part of the stick represents the number of customers sitting at that specific table in the CRP process. It can be seen that the higher values of alpha lead to realizations that are closer to the base distribution. It is clear that realizations of a Dirichlet process are discrete distributions.}}{16}{figure.5}}
\newlabel{crpGausBase}{{5}{16}{The stick breaking process. Every part of the stick represents the number of customers sitting at that specific table in the CRP process. It can be seen that the higher values of alpha lead to realizations that are closer to the base distribution. It is clear that realizations of a Dirichlet process are discrete distributions}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mixture model}}{16}{figure.6}}
\newlabel{1dGMM}{{6}{16}{Mixture model}{figure.6}{}}
\citation{antoniak}
\citation{caron}
\citation{caron}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Inference}{17}{subsection.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A Dirichlet process mixture model.}}{17}{figure.7}}
\newlabel{mm}{{7}{17}{A Dirichlet process mixture model}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Generalized Polya Urn}{17}{subsection.3.7}}
\citation{caron}
\newlabel{generalPolya}{{3.7}{18}{Generalized Polya Urn}{subsection.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces GPU as a function of CRP}}{18}{figure.8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GPU}}{18}{algorithm.1}}
\newlabel{GPU}{{1}{18}{Generalized Polya Urn}{algorithm.1}{}}
\newlabel{gpu:algo}{{1}{18}{Generalized Polya Urn}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Model definition}{19}{section.4}}
\newlabel{sec:model}{{4}{19}{Model definition}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}General pipeline}{19}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces General landmark update pipeline}}{19}{figure.9}}
\newlabel{pipeline}{{9}{19}{General landmark update pipeline}{figure.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Landmark Layer}}{19}{algorithm.2}}
\newlabel{euclid}{{2}{19}{General pipeline}{algorithm.2}{}}
\citation{pcl}
\citation{objectpointSLAM}
\citation{fpfh}
\citation{smcddp}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Point cloud modification pipeline.}}{20}{figure.10}}
\newlabel{pcl:mod}{{10}{20}{Point cloud modification pipeline}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The data distribution}{20}{subsection.4.2}}
\newlabel{data:dist}{{4.2}{20}{The data distribution}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Exponential trend}}{20}{figure.11}}
\newlabel{pcl:kl}{{11}{20}{Exponential trend}{figure.11}{}}
\citation{caron}
\citation{smc:theory}
\citation{doucet}
\newlabel{eq1}{{14}{22}{The data distribution}{equation.4.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Sequential monte carlo sampler}{22}{subsection.4.3}}
\newlabel{smcsampler}{{4.3}{22}{Sequential monte carlo sampler}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Gibbs updates}{22}{subsubsection.4.3.1}}
\newlabel{gibbsUpd}{{4.3.1}{22}{Gibbs updates}{subsubsection.4.3.1}{}}
\newlabel{Gibbs}{{16}{22}{Gibbs updates}{equation.4.16}{}}
\citation{compendium}
\citation{compendium}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces SMC for DDPM}}{23}{algorithm.3}}
\newlabel{SMC}{{3}{23}{Sequential monte carlo sampler}{algorithm.3}{}}
\newlabel{Q1}{{17}{23}{Gibbs updates}{equation.4.17}{}}
\citation{conjugate}
\newlabel{Q_2}{{18}{24}{Gibbs updates}{equation.4.18}{}}
\newlabel{udpates}{{19}{24}{Gibbs updates}{equation.4.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Weight updates}{24}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Decision Layer}{24}{subsection.4.4}}
\citation{RANSAC}
\citation{fpfh}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Complexity}{25}{subsection.4.5}}
\newlabel{subsec:complexity}{{4.5}{25}{Complexity}{subsection.4.5}{}}
\newlabel{Q_filt}{{22}{25}{Complexity}{equation.4.22}{}}
\citation{smcddp}
\citation{sqlite}
\newlabel{complexity}{{23}{26}{Complexity}{equation.4.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Landmark size}{26}{subsection.4.6}}
\citation{pcl}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{27}{section.5}}
\newlabel{sec:results}{{5}{27}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Simple datasets}{27}{subsection.5.1}}
\newlabel{distancesofclusters}{{5.1}{27}{Simple datasets}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Initial data along with the distributions infered }}{28}{figure.12}}
\newlabel{pcl:clust}{{12}{28}{Initial data along with the distributions infered}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces More complicated color distributions}}{29}{figure.13}}
\newlabel{pcl:clust2}{{13}{29}{More complicated color distributions}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The colour and position boundary is displayed in these pictures}}{30}{figure.14}}
\newlabel{pip:bounds}{{14}{30}{The colour and position boundary is displayed in these pictures}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Expressivensess and decision layer}{30}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}EKF-SLAM experiments}{31}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Speed}{31}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces SLAM session using the pipeline. The pipeline is used as the sensor model of an EKF-SLAM module. The readings of a kinect mounted on a turtlebot are downsampled and clustered. Current readings are either being matched to past readings giving old landmarks or being used to create new landmarks if no similar past environment sigantures exist. Landmarks are represented with yellow spheres. }}{32}{figure.15}}
\newlabel{SLAM}{{15}{32}{SLAM session using the pipeline. The pipeline is used as the sensor model of an EKF-SLAM module. The readings of a kinect mounted on a turtlebot are downsampled and clustered. Current readings are either being matched to past readings giving old landmarks or being used to create new landmarks if no similar past environment sigantures exist. Landmarks are represented with yellow spheres}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Memory requirements}{32}{subsection.5.5}}
\newlabel{reqs}{{5.5}{32}{Memory requirements}{subsection.5.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Benchmark of the pipeline using different downsampling settings on two different machines.}}{33}{table.1}}
\newlabel{bench}{{1}{33}{Benchmark of the pipeline using different downsampling settings on two different machines}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Memory Requirements as a function of strength parameter $\alpha $.}}{33}{figure.16}}
\newlabel{memReq}{{16}{33}{Memory Requirements as a function of strength parameter $\alpha $}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Result of a session using RTAB-map module. The resulting map captures the structure of the room and the maps for the room shown in the picture average 84MB in size.}}{34}{figure.17}}
\newlabel{rtabmap}{{17}{34}{Result of a session using RTAB-map module. The resulting map captures the structure of the room and the maps for the room shown in the picture average 84MB in size}{figure.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{35}{section.6}}
\newlabel{sec:discussion}{{6}{35}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Unsupervised learning}{35}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Unsupervised entity extraction.}}{35}{figure.18}}
\newlabel{pip:beh}{{18}{35}{Unsupervised entity extraction}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Clustering layer}{35}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Cases where using very low/high values on the hyper parameter $\alpha $ lead to a pipeline that either groups the whole cloud being to a single landmark or to a pipeline that constantly creates new landmarks.}}{36}{figure.19}}
\newlabel{pip:limits}{{19}{36}{Cases where using very low/high values on the hyper parameter $\alpha $ lead to a pipeline that either groups the whole cloud being to a single landmark or to a pipeline that constantly creates new landmarks}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Decision layer}{37}{subsection.6.3}}
\newlabel{pip:expo}{{6.3}{37}{Decision layer}{subsection.6.3}{}}
\citation{dependentDiri}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Downsampling and Filtering}{38}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The behaviour of the dowsampling module with respect to the threshold of the operations.}}{38}{figure.20}}
\newlabel{pip:downsample}{{20}{38}{The behaviour of the dowsampling module with respect to the threshold of the operations}{figure.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Data distribution}{38}{subsection.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Scalability}{39}{subsection.6.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Memory requirements with respect to the number of landmarks in the environment.}}{39}{figure.21}}
\newlabel{pip:reqs}{{21}{39}{Memory requirements with respect to the number of landmarks in the environment}{figure.21}{}}
\bibcite{probRobs}{1}
\bibcite{ekf}{2}
\bibcite{graph}{3}
\bibcite{Figueredo:2009dg}{4}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and future work}{40}{section.7}}
\newlabel{sec:conclusion}{{7}{40}{Conclusion and future work}{section.7}{}}
\bibcite{liflonglearning}{5}
\bibcite{lifelongmaps}{6}
\bibcite{aishalong}{7}
\bibcite{bayesianNon}{8}
\bibcite{dependent}{9}
\bibcite{brml}{10}
\bibcite{dependentDiri}{11}
\bibcite{pcl}{12}
\bibcite{rtabmap}{13}
\bibcite{SLAM++}{14}
\bibcite{objSLAM}{15}
\bibcite{castleetal}{16}
\bibcite{objectpointSLAM}{17}
\bibcite{objectpoint}{18}
\bibcite{monoSLAM}{19}
\bibcite{objectDisc}{20}
\bibcite{distMes}{21}
\bibcite{fpfh}{22}
\bibcite{segOverview}{23}
\bibcite{gpu}{24}
\bibcite{kinect}{25}
\bibcite{nonParam}{26}
\bibcite{omnimaper}{27}
\bibcite{imft}{28}
\bibcite{pointSeg}{29}
\bibcite{planarSeg}{30}
\bibcite{planarSeg2}{31}
\bibcite{smartSeg}{32}
\bibcite{smcddp}{33}
\bibcite{corresp:first}{34}
\bibcite{corres:sec}{35}
\bibcite{corres:three}{36}
\bibcite{corres:four}{37}
\bibcite{bayes:neal}{38}
\bibcite{bayes:jordan}{39}
\bibcite{SLAM}{40}
\bibcite{bayes:hier}{41}
\bibcite{bayes:smc}{42}
\bibcite{LDA}{43}
\bibcite{theory:ddp}{44}
\bibcite{speakerDiar}{45}
\bibcite{antoniak}{46}
\bibcite{caron}{47}
\bibcite{compendium}{48}
\bibcite{smc:theory}{49}
\bibcite{doucet}{50}
\bibcite{RANSAC}{51}
\bibcite{conjugate}{52}
\bibcite{sqlite}{53}
\bibcite{polya}{54}
\bibcite{infants}{55}
\bibcite{rgbdmapping}{56}
